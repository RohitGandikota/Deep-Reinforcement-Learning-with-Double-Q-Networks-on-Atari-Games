{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41763e5d",
   "metadata": {},
   "source": [
    "# Installations\n",
    "\n",
    "The following steps can be considered before installing gym and atari-py in Windows Anaconda. [Linux Installation is straight forward]\n",
    "\n",
    "Step 1: Create a new environment in anaconda: \n",
    "```\n",
    "conda create -n <env_name> python=3.9\n",
    "conda activate <env_name?\n",
    "```\n",
    "\n",
    "Step 2: [Depends on Visual Studio]. If Visual Studio is not present please download and install\n",
    "   - Download VS build tools [here](https://visualstudio.microsoft.com/thank-you-downloading-visual-studio/?sku=BuildTools&rel=16)\n",
    "   - Run the VS build setup and select \"C++ build tools\" and install it.\n",
    "\n",
    "Step 3: Packages Installation in the created environment [Using Pip]\n",
    "```\n",
    "pip install tensorflow\n",
    "pip install cmake\n",
    "pip install atari-py\n",
    "pip install gym\n",
    "pip install gym[atari]\n",
    "pip install keras-rl2\n",
    "```\n",
    "\n",
    "Step 4: With the latest atari-py verions, only `Tetris` game is available. To get all the games:\n",
    "   - Download the ROMS from [this link](http://www.atarimania.com/rom_collection_archive_atari_2600_roms.html)\n",
    "   - Unrar the folder at any location\n",
    "   - Run the below code in the conda prompt\n",
    "```\n",
    "python -m atari_py.import_roms <path to folder where ROMS are unrared>\n",
    "```\n",
    "\n",
    "Step 5: Enjoy Coding !!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9569c0",
   "metadata": {},
   "source": [
    "### Imports and listing down the available games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c26c0dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adventure', 'air_raid', 'alien', 'amidar', 'assault', 'asterix', 'asteroids', 'atlantis', 'bank_heist', 'battle_zone', 'beam_rider', 'berzerk', 'bowling', 'boxing', 'breakout', 'carnival', 'centipede', 'chopper_command', 'crazy_climber', 'defender', 'demon_attack', 'donkey_kong', 'double_dunk', 'elevator_action', 'enduro', 'fishing_derby', 'freeway', 'frogger', 'frostbite', 'galaxian', 'gopher', 'gravitar', 'hero', 'ice_hockey', 'jamesbond', 'journey_escape', 'kaboom', 'kangaroo', 'keystone_kapers', 'king_kong', 'koolaid', 'krull', 'kung_fu_master', 'laser_gates', 'lost_luggage', 'montezuma_revenge', 'mr_do', 'ms_pacman', 'name_this_game', 'pacman', 'phoenix', 'pitfall', 'pong', 'pooyan', 'private_eye', 'qbert', 'riverraid', 'road_runner', 'robotank', 'seaquest', 'sir_lancelot', 'skiing', 'solaris', 'space_invaders', 'star_gunner', 'surround', 'tennis', 'tetris', 'time_pilot', 'trondead', 'tutankham', 'up_n_down', 'venture', 'video_pinball', 'wizard_of_wor', 'yars_revenge', 'zaxxon']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rohit\\Anaconda3\\envs\\openai\\lib\\site-packages\\ale_py\\roms\\__init__.py:89: DeprecationWarning: Automatic importing of atari-py roms won't be supported in future releases of ale-py. Please migrate over to using `ale-import-roms` OR an ALE-supported ROM package. To make this warning disappear you can run `ale-import-roms --import-from-pkg atari_py.atari_roms`.For more information see: https://github.com/mgbellemare/Arcade-Learning-Environment#rom-management\n",
      "  ROMS = resolve_roms()\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import random\n",
    "import atari_py\n",
    "import numpy as np\n",
    "print(atari_py.list_games())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311f849f",
   "metadata": {},
   "source": [
    "### Loading the game and exploring the actions and observation space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77235f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rohit\\Anaconda3\\envs\\openai\\lib\\site-packages\\gym\\envs\\registration.py:564: UserWarning: \u001b[33mWARN: The environment SpaceInvaders-v0 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game Frame Dimensions: [210,160,3]\n",
      "Number of Actions in Game: 6\n",
      "Details of available actions in Game: ['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rohit\\Anaconda3\\envs\\openai\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:97: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('SpaceInvaders-v0')\n",
    "height, width, channels = env.observation_space.shape\n",
    "actions = env.action_space.n\n",
    "\n",
    "print(f'Game Frame Dimensions: [{height},{width},{channels}]')\n",
    "print(f'Number of Actions in Game: {actions}')\n",
    "print(f'Details of available actions in Game: {env.unwrapped.get_action_meanings()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a4c0a0",
   "metadata": {},
   "source": [
    "### Taking random actions and observing the state and score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4cdac3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rohit\\Anaconda3\\envs\\openai\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:305: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 -> Score 80.0\n",
      "Episode 1 -> Score 155.0\n",
      "Episode 2 -> Score 100.0\n",
      "Episode 3 -> Score 110.0\n",
      "Episode 4 -> Score 75.0\n"
     ]
    }
   ],
   "source": [
    "episodes = 5\n",
    "\n",
    "for episode in range(episodes):\n",
    "    score = 0\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    while not done:\n",
    "        # If you need the game frame as input use this render option (it outputs rgb image of the current state of the env)\n",
    "        frame = env.render(mode='rgb_array')\n",
    "        # Take random action\n",
    "        action_to_take = random.choice([0,1,2,3,4,5])\n",
    "        new_state, reward, done, info = env.step(action_to_take)\n",
    "        score+=reward\n",
    "    print(f'Episode {episode} -> Score {score}')\n",
    "    \n",
    "env.reset()\n",
    "env.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdc9c3a",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning\n",
    "With deep reinforcement learning, we use deep neural networks to estimate the q-values that are used to take an action (used in policy). In many practical decision making problems, the states s {\\displaystyle s} s of the MDP are high-dimensional (eg. images from a camera or the raw sensor stream from a robot) and cannot be solved by traditional RL algorithms. Deep reinforcement learning algorithms incorporate deep learning to solve such Maps a, often representing the policy Ï€ ( a | s ) or other learned functions as a neural network, and developing specialized algorithms that perform well in this setting. \n",
    "## Building a conv network for estimating Q values (Deep RL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ee7c6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell after trying q_net.compile() and your system is crashing !!! [or] if you are getting AttributeError: 'Functional' object has no attribute '_compile_time_distribution_strategy'\n",
    "del q_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c5282d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rohit\\Anaconda3\\envs\\openai\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 3, 210, 160, 3)]  0         \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 3, 105, 80, 8)     392       \n",
      "                                                                 \n",
      " leaky_re_lu_5 (LeakyReLU)   (None, 3, 105, 80, 8)     0         \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 3, 53, 40, 16)     2064      \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 3, 53, 40, 16)    64        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_6 (LeakyReLU)   (None, 3, 53, 40, 16)     0         \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 3, 27, 20, 32)     8224      \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 3, 27, 20, 32)    128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_7 (LeakyReLU)   (None, 3, 27, 20, 32)     0         \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 3, 14, 10, 64)     32832     \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 3, 14, 10, 64)    256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_8 (LeakyReLU)   (None, 3, 14, 10, 64)     0         \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 3, 7, 5, 64)       65600     \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 3, 7, 5, 64)      256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_9 (LeakyReLU)   (None, 3, 7, 5, 64)       0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 6720)              0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 512)               3441152   \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 128)               65664     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 6)                 774       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,617,406\n",
      "Trainable params: 3,617,054\n",
      "Non-trainable params: 352\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "\n",
    "def QNet(height,width,channels, actions):\n",
    "    # weight initialization\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    # source image input\n",
    "    in_src_image = Input(shape=(3,height,width,channels))\n",
    "    # C8\n",
    "    d = Conv2D(8, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(in_src_image)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    # C16\n",
    "    d = Conv2D(16, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
    "    d = BatchNormalization()(d)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    # C32\n",
    "    d = Conv2D(32, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
    "    d = BatchNormalization()(d)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    # C64\n",
    "    d = Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
    "    d = BatchNormalization()(d)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    # C64\n",
    "    d = Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
    "    d = BatchNormalization()(d)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    # second last output layer\n",
    "    d = Flatten()(d)\n",
    "    d = Dense(512, activation='relu')(d)\n",
    "    d = Dense(128, activation='relu')(d)\n",
    "    out = Dense(actions, activation='softmax')(d)\n",
    "    \n",
    "    # define model\n",
    "    model = Model(in_src_image, out)\n",
    "    # compile model\n",
    "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, loss_weights=[0.5])\n",
    "    return model\n",
    "\n",
    "q_net = QNet(height,width,channels, actions)\n",
    "q_net.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a01afe",
   "metadata": {},
   "source": [
    "## Building Agent for Double Deep Q Network Reinforcement Learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1876694f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.agents import DQNAgent\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "850b96b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(model, actions):\n",
    "    policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1., value_min=.1, value_test=.2, nb_steps=10000)\n",
    "    memory = SequentialMemory(limit=1000, window_length=3)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy,\n",
    "                  enable_dueling_network=False, dueling_type='avg', \n",
    "                   nb_actions=actions, nb_steps_warmup=1000\n",
    "                  )\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "78124bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = build_agent(q_net, actions)\n",
    "dqn.get_config()\n",
    "dqn.compile(Adam(lr=1e-4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "98827184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 10000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rohit\\Anaconda3\\envs\\openai\\lib\\site-packages\\keras\\engine\\training_v1.py:2067: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  467/10000: episode: 1, duration: 8.824s, episode steps: 467, steps per second:  53, episode reward: 60.000, mean reward:  0.128 [ 0.000, 15.000], mean action: 2.368 [0.000, 5.000],  loss: --, mean_q: --, mean_eps: --\n",
      "  880/10000: episode: 2, duration: 6.193s, episode steps: 413, steps per second:  67, episode reward: 55.000, mean reward:  0.133 [ 0.000, 20.000], mean action: 2.516 [0.000, 5.000],  loss: --, mean_q: --, mean_eps: --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rohit\\Anaconda3\\envs\\openai\\lib\\site-packages\\keras\\engine\\training_v1.py:2067: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2434/10000: episode: 3, duration: 1435.656s, episode steps: 1554, steps per second:   1, episode reward: 700.000, mean reward:  0.450 [ 0.000, 200.000], mean action: 2.380 [0.000, 5.000],  loss: 10.158795, mean_q: 0.602062, mean_eps: 0.845470\n",
      " 3136/10000: episode: 4, duration: 681.926s, episode steps: 702, steps per second:   1, episode reward: 105.000, mean reward:  0.150 [ 0.000, 30.000], mean action: 2.497 [0.000, 5.000],  loss: 10.227453, mean_q: 0.462577, mean_eps: 0.749395\n",
      " 4223/10000: episode: 5, duration: 1046.962s, episode steps: 1087, steps per second:   1, episode reward: 190.000, mean reward:  0.175 [ 0.000, 30.000], mean action: 2.443 [0.000, 5.000],  loss: 1.358039, mean_q: 0.475294, mean_eps: 0.668890\n",
      " 4700/10000: episode: 6, duration: 461.013s, episode steps: 477, steps per second:   1, episode reward: 65.000, mean reward:  0.136 [ 0.000, 30.000], mean action: 2.472 [0.000, 5.000],  loss: 2.025151, mean_q: 0.413718, mean_eps: 0.598510\n",
      " 5694/10000: episode: 7, duration: 961.805s, episode steps: 994, steps per second:   1, episode reward: 245.000, mean reward:  0.246 [ 0.000, 30.000], mean action: 2.416 [0.000, 5.000],  loss: 1.991289, mean_q: 0.474450, mean_eps: 0.532315\n",
      " 6196/10000: episode: 8, duration: 485.719s, episode steps: 502, steps per second:   1, episode reward: 50.000, mean reward:  0.100 [ 0.000, 15.000], mean action: 2.219 [0.000, 5.000],  loss: 2.152787, mean_q: 0.354036, mean_eps: 0.464995\n",
      " 7026/10000: episode: 9, duration: 801.912s, episode steps: 830, steps per second:   1, episode reward: 140.000, mean reward:  0.169 [ 0.000, 30.000], mean action: 2.287 [0.000, 5.000],  loss: 1.091872, mean_q: 0.372939, mean_eps: 0.405055\n",
      " 8423/10000: episode: 10, duration: 1348.183s, episode steps: 1397, steps per second:   1, episode reward: 240.000, mean reward:  0.172 [ 0.000, 30.000], mean action: 2.369 [0.000, 5.000],  loss: 1.511466, mean_q: 0.318592, mean_eps: 0.304840\n",
      " 8826/10000: episode: 11, duration: 390.305s, episode steps: 403, steps per second:   1, episode reward: 110.000, mean reward:  0.273 [ 0.000, 25.000], mean action: 2.561 [0.000, 5.000],  loss: 1.928315, mean_q: 0.294804, mean_eps: 0.223840\n",
      " 9494/10000: episode: 12, duration: 646.602s, episode steps: 668, steps per second:   1, episode reward: 105.000, mean reward:  0.157 [ 0.000, 25.000], mean action: 2.169 [0.000, 5.000],  loss: 1.590774, mean_q: 0.315554, mean_eps: 0.175645\n",
      " 9960/10000: episode: 13, duration: 451.708s, episode steps: 466, steps per second:   1, episode reward: 55.000, mean reward:  0.118 [ 0.000, 30.000], mean action: 2.369 [0.000, 5.000],  loss: 1.247391, mean_q: 0.288351, mean_eps: 0.124615\n",
      "done, took 8765.485 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ba3f83e700>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.fit(env, nb_steps=10000, visualize=False, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f40c357d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.save_weights('D:\\\\Projects\\\\DeepRL\\\\dqn_spaceinvadersv0-weights.h5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0dbe9623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your model with the saved weights\n",
    "del q_net,dqn\n",
    "q_net = QNet(height,width,channels, actions)\n",
    "dqn = build_agent(q_net, actions)\n",
    "dqn.get_config()\n",
    "dqn.compile(Adam(lr=1e-4))\n",
    "dqn.load_weights('D:\\\\Projects\\\\DeepRL\\\\dqn_spaceinvadersv0-weights.h5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "99daeb44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 2 episodes ...\n",
      "Episode 1: reward: 120.000, steps: 625\n",
      "Episode 2: reward: 185.000, steps: 716\n",
      "152.5\n"
     ]
    }
   ],
   "source": [
    "# To visualize the results, initiate the env with render_mode again\n",
    "env = gym.make('SpaceInvaders-v4', render_mode='human')\n",
    "scores = dqn.test(env, nb_episodes=2, visualize=False)\n",
    "print(np.mean(scores.history['episode_reward']))\n",
    "env.reset()\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "openai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
